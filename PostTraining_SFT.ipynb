{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc935f2b-a58b-4110-8918-96c868047b38",
   "metadata": {},
   "source": [
    "# Post Training - Supervised Fine-Tuning (SFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d76b86a7-7a45-4749-866e-0d561041bce1",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbfc986-9ac7-4a2d-9dd0-a76841c7f46d",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3304e49d-bd1e-469b-a5b4-5edb16ecf344",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM, SFTConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc63b02-5e9a-4a83-b042-4a2386cf5976",
   "metadata": {},
   "source": [
    "## Setting up helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69982ae0-755e-48cf-ba4c-3b83b091fd9a",
   "metadata": {
    "height": 625
   },
   "outputs": [],
   "source": [
    "def generate_responses(model, tokenizer, user_message, system_message=None, \n",
    "                       max_new_tokens=100):\n",
    "    #Formating chat using tokenizer's chat template:\n",
    "    #Preparing a list of chat messages (structured format):\n",
    "    messages = []\n",
    "    \n",
    "    #If a system message is provided, adding it first:\n",
    "    #System messages define assistant behavior (e.g., tone, personality):\n",
    "    if system_message:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "    \n",
    "    #Add the user message as the next entry (it's a single-turn chat setup):\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    \n",
    "    #Tokenizing the prompt into input IDs and move to the model's device (CPU or GPU):\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False, #Return raw text prompt, not tokenized output.\n",
    "        add_generation_prompt=True, #Add assistant's cue to prompt generation.\n",
    "        enable_thinking=False, #Optional setting (used in some chat-aware models).\n",
    "    )\n",
    "    \n",
    "    #Disabling gradient calculation to save memory (inference-only):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    #Recommended to use vllm, sglang or TensorRT (For trying different menthods for inference):\n",
    "    with torch.no_grad():\n",
    "        #Generating output tokens from the model:\n",
    "        outputs = model.generate(\n",
    "            **inputs,   #Using a double pointer for unpacking the dictionary of inputs (model.generate(**inputs)) that is equivalent to (model.generate(input_ids=..., attention_mask=...)).\n",
    "            max_new_tokens=max_new_tokens, #Limit the number of tokens generated.\n",
    "            do_sample=False, #Disabling randomness (greedy decoding).\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    input_len = inputs[\"input_ids\"].shape[1] #Getting the length of the input (so we can extract only the newly generated tokens).\n",
    "    generated_ids = outputs[0][input_len:] #Slicing the output to keep only the new tokens (assistant's response).\n",
    "    \n",
    "    #Decoding the generated token IDs back into text:\n",
    "    #`skip_special_tokens=True` removing tokens like <|endoftext|>\n",
    "    #Strip() removes any leading/trailing whitespace or newline characters from the output string to keeps the model output clean and ready to display or use.\n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "234e5b05-a493-4683-91fd-7417885efc0f",
   "metadata": {
    "height": 234
   },
   "outputs": [],
   "source": [
    "def test_model_with_questions(model, tokenizer, questions, \n",
    "                              system_message=None, title=\"Model Output\"):\n",
    "    #Printing section title for clarity (e.g., \"Base Model (Before SFT) Output\")\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    \n",
    "    #Looping through each question in the list, starting index at 1:\n",
    "    for i, question in enumerate(questions, 1):\n",
    "        #Generating a model response for the current question:\n",
    "        #Passing in the question as user input and optional system message\n",
    "        response = generate_responses(model, tokenizer, question, \n",
    "                                      system_message)\n",
    "        #Print both the input question and the model's output response:\n",
    "        print(f\"\\nModel Input {i}:\\n{question}\\nModel Output {i}:\\n{response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c273931-6827-4ee1-af1a-83a99bf94bf7",
   "metadata": {
    "height": 489
   },
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name, use_gpu = False):\n",
    "    \n",
    "    #Loading tokenizer from the given model path or HuggingFace Hub name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    #Loading causal language model (this is a GPT-style decoder-only model):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    #If GPU is requested and available, move the model to CUDA:\n",
    "    if use_gpu:\n",
    "        model.to(\"cuda\")\n",
    "    \n",
    "    #If the tokenizer does not already have a chat template, defined a custom one:\n",
    "    #This template is used to format multi-turn conversations into a prompt string:\n",
    "    if not tokenizer.chat_template:\n",
    "        tokenizer.chat_template = \"\"\"{% for message in messages %}\n",
    "                {% if message['role'] == 'system' %}System: {{ message['content'] }}\\n\n",
    "                {% elif message['role'] == 'user' %}User: {{ message['content'] }}\\n\n",
    "                {% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }} <|endoftext|>\n",
    "                {% endif %}\n",
    "                {% endfor %}\"\"\"\n",
    "    \n",
    "    #Ensuring tokenizer has a pad token — fallback to eos token if missing:\n",
    "    if not tokenizer.pad_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    #Returning the ready-to-use model and tokenizer:   \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bd15e1d-6ecd-4337-a5dd-1602da354f62",
   "metadata": {
    "height": 319
   },
   "outputs": [],
   "source": [
    "def display_dataset(dataset):\n",
    "    # Visualize the dataset \n",
    "    rows = []\n",
    "    for i in range(3):\n",
    "        example = dataset[i]\n",
    "        print(\"*\"*20)\n",
    "        print(f\"example {i} : {example}\\n\")\n",
    "        user_msg = next(m['content'] for m in example['messages']\n",
    "                        if m['role'] == 'user')\n",
    "        print(f\"user_msg: {user_msg} \\n\")\n",
    "        assistant_msg = next(m['content'] for m in example['messages']\n",
    "                             if m['role'] == 'assistant')\n",
    "        print(f\"assistant_msg: {assistant_msg} \\n\")\n",
    "        \n",
    "        #Append a dictionary with user and assistant messages to the rows list:\n",
    "        rows.append({\n",
    "            'User Prompt': user_msg,\n",
    "            'Assistant Response': assistant_msg\n",
    "        })\n",
    "        print(f\"rows: {rows} \\n\\n\")\n",
    "    \n",
    "    #Display as table:\n",
    "    #Convert the list of dictionaries into a pandas DataFrame:\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    #Ensuring full text is shown without truncation in display:\n",
    "    pd.set_option('display.max_colwidth', None)  #Avoiding truncating long strings.\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ac817-43a4-43c9-88f3-9825b96b84b7",
   "metadata": {},
   "source": [
    "## Load base model & test on simple questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fed78c2-ea93-4ac2-bd6f-5d4391de7c8d",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "USE_GPU = False\n",
    "\n",
    "questions = [\n",
    "    \"Give me an 1-sentence introduction of LLM.\",\n",
    "    \"Calculate 1+1-1\",\n",
    "    \"What's the difference between thread and process?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba426c74-4d93-42b3-b2c7-5791fb9bf3c5",
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Base Model (Before SFT) Output ===\n",
      "\n",
      "Model Input 1:\n",
      "Give me an 1-sentence introduction of LLM.\n",
      "Model Output 1:\n",
      "⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ �\n",
      "\n",
      "\n",
      "Model Input 2:\n",
      "Calculate 1+1-1\n",
      "Model Output 2:\n",
      "⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ �\n",
      "\n",
      "\n",
      "Model Input 3:\n",
      "What's the difference between thread and process?\n",
      "Model Output 3:\n",
      "⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ �\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model_and_tokenizer(\"./models/Qwen/Qwen3-0.6B-Base\", USE_GPU)\n",
    "\n",
    "test_model_with_questions(model, tokenizer, questions, \n",
    "                          title=\"Base Model (Before SFT) Output\")\n",
    "\n",
    "del model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "043ff317",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Give me an 1-sentence introduction of LLM.\n",
      "1\n",
      "Calculate 1+1-1\n",
      "2\n",
      "What's the difference between thread and process?\n"
     ]
    }
   ],
   "source": [
    "for i, question in enumerate(questions):\n",
    "    print(i)\n",
    "    print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9edb8921",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Give me an 1-sentence introduction of LLM.\n",
      "2\n",
      "Calculate 1+1-1\n",
      "3\n",
      "What's the difference between thread and process?\n"
     ]
    }
   ],
   "source": [
    "for i, question in enumerate(questions,1):\n",
    "    print(i)\n",
    "    print(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d7d32a",
   "metadata": {},
   "source": [
    "### `enumerate(questions)` vs `enumerate(questions, 1)`\n",
    "\n",
    "When using the `enumerate()` function in Python, you can control where the index starts from by providing a second argument.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example 1: `enumerate(questions, 1)`\n",
    "\n",
    "```python\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(i)\n",
    "    print(question)\n",
    "````\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "1\n",
    "Give me an 1-sentence introduction of LLM.\n",
    "2\n",
    "Calculate 1+1-1\n",
    "3\n",
    "What's the difference between thread and process?\n",
    "```\n",
    "\n",
    "* The index `i` starts from **1**.\n",
    "* Useful for **human-readable output**, like \"Question 1\", \"Model Output 2\", etc.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example 2: `enumerate(questions)`\n",
    "\n",
    "```python\n",
    "for i, question in enumerate(questions):\n",
    "    print(i)\n",
    "    print(question)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "0\n",
    "Give me an 1-sentence introduction of LLM.\n",
    "1\n",
    "Calculate 1+1-1\n",
    "2\n",
    "What's the difference between thread and process?\n",
    "```\n",
    "\n",
    "* The index `i` starts from **0** (default behavior).\n",
    "* Useful when zero-based indexing matters (e.g., list positions, array access).\n",
    "\n",
    "---\n",
    "\n",
    "### Key Points\n",
    "\n",
    "| Expression                | Index Starts From | Skips Anything? | Use Case                       |\n",
    "| ------------------------- | ----------------- | --------------- | ------------------------------ |\n",
    "| `enumerate(questions)`    | 0                 | ❌ No            | Programming logic, loops       |\n",
    "| `enumerate(questions, 1)` | 1                 | ❌ No            | Human-friendly display/logging |\n",
    "\n",
    "> Changing the start index does **not** skip or change the list items — it only affects the index value you get during iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885253e0-3a9a-4b42-a36b-a8a3ddd340a1",
   "metadata": {},
   "source": [
    "## SFT results on Qwen3-0.6B model\n",
    "\n",
    "We are reviewing the results of a previously completed SFT training. Due to limited resources, we won’t be running the full training on a relatively large model like Qwen3-0.6B. \n",
    "However, in the next section of this notebook, you’ll walk through the full training process using a smaller model and a lightweight dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e86f13c-c969-4c7e-8702-d074ee7a2ce6",
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Base Model (After SFT) Output ===\n",
      "\n",
      "Model Input 1:\n",
      "Give me an 1-sentence introduction of LLM.\n",
      "Model Output 1:\n",
      "LLM is a program that provides advanced legal knowledge and skills to professionals and individuals.\n",
      "\n",
      "\n",
      "Model Input 2:\n",
      "Calculate 1+1-1\n",
      "Model Output 2:\n",
      "1+1-1 = 2-1 = 1\n",
      "\n",
      "So, the final answer is 1.\n",
      "\n",
      "\n",
      "Model Input 3:\n",
      "What's the difference between thread and process?\n",
      "Model Output 3:\n",
      "In computer science, a thread is a unit of execution that runs in a separate process. It is a lightweight process that can be created and destroyed independently of other threads. Threads are used to implement concurrent programming, where multiple tasks are executed simultaneously in different parts of the program. Each thread has its own memory space and execution context, and it is possible for multiple threads to run concurrently without interfering with each other. Threads are also known as lightweight processes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model_and_tokenizer(\"./models/banghua/Qwen3-0.6B-SFT\", USE_GPU)\n",
    "\n",
    "test_model_with_questions(model, tokenizer, questions, \n",
    "                          title=\"Base Model (After SFT) Output\")\n",
    "\n",
    "del model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf299ee-aa84-4c43-8d7b-f0998077e2cb",
   "metadata": {},
   "source": [
    "## Doing SFT on a small model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5cb7ea-e157-418e-84f5-34ecbed823ad",
   "metadata": {},
   "source": [
    "Performing SFT on a small model <code>HuggingFaceTB/SmolLM2-135M</code> and a smaller training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdb07589-049d-432e-8001-e6e9175ad806",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "model_name = \"./models/HuggingFaceTB/SmolLM2-135M\"\n",
    "model, tokenizer = load_model_and_tokenizer(model_name, USE_GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20d52c3e-9c6c-43c3-bd95-92d60b9c9a8f",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "example 0 : {'messages': [{'content': \"- The left child should have a value less than the parent node's value, and the right child should have a value greater than the parent node's value.\", 'role': 'user'}, {'content': \"This statement is correct. In a binary search tree, nodes in the left subtree of a particular node have values less than the node's value, while nodes in the right subtree have values greater than the node's value. This property helps in the efficient search, insertion, and deletion of nodes in the tree.\", 'role': 'assistant'}]}\n",
      "\n",
      "user_msg: - The left child should have a value less than the parent node's value, and the right child should have a value greater than the parent node's value. \n",
      "\n",
      "assistant_msg: This statement is correct. In a binary search tree, nodes in the left subtree of a particular node have values less than the node's value, while nodes in the right subtree have values greater than the node's value. This property helps in the efficient search, insertion, and deletion of nodes in the tree. \n",
      "\n",
      "rows: [{'User Prompt': \"- The left child should have a value less than the parent node's value, and the right child should have a value greater than the parent node's value.\", 'Assistant Response': \"This statement is correct. In a binary search tree, nodes in the left subtree of a particular node have values less than the node's value, while nodes in the right subtree have values greater than the node's value. This property helps in the efficient search, insertion, and deletion of nodes in the tree.\"}] \n",
      "\n",
      "\n",
      "********************\n",
      "example 1 : {'messages': [{'content': 'To pass three levels must be the plan.\\nThen tackle Two, when that is done.\\nOf 100 that start, at the end will be 20.\\nFinQuiz is a website that helps you prepare.\\nUse it to be stress-free, and not lose your hair.\\nThen, take the exam with a smile on your face.\\nBe confident that you will gain your place.\\nSo make this the goal to which you aspire. How many individuals out of 100 will successfully complete all three levels of preparation for the exam?', 'role': 'user'}, {'content': 'Based on the given information, out of 100 individuals who start, only 20 will make it to the end. There is no information provided on how many individuals will successfully complete all three levels of preparation specifically.', 'role': 'assistant'}]}\n",
      "\n",
      "user_msg: To pass three levels must be the plan.\n",
      "Then tackle Two, when that is done.\n",
      "Of 100 that start, at the end will be 20.\n",
      "FinQuiz is a website that helps you prepare.\n",
      "Use it to be stress-free, and not lose your hair.\n",
      "Then, take the exam with a smile on your face.\n",
      "Be confident that you will gain your place.\n",
      "So make this the goal to which you aspire. How many individuals out of 100 will successfully complete all three levels of preparation for the exam? \n",
      "\n",
      "assistant_msg: Based on the given information, out of 100 individuals who start, only 20 will make it to the end. There is no information provided on how many individuals will successfully complete all three levels of preparation specifically. \n",
      "\n",
      "rows: [{'User Prompt': \"- The left child should have a value less than the parent node's value, and the right child should have a value greater than the parent node's value.\", 'Assistant Response': \"This statement is correct. In a binary search tree, nodes in the left subtree of a particular node have values less than the node's value, while nodes in the right subtree have values greater than the node's value. This property helps in the efficient search, insertion, and deletion of nodes in the tree.\"}, {'User Prompt': 'To pass three levels must be the plan.\\nThen tackle Two, when that is done.\\nOf 100 that start, at the end will be 20.\\nFinQuiz is a website that helps you prepare.\\nUse it to be stress-free, and not lose your hair.\\nThen, take the exam with a smile on your face.\\nBe confident that you will gain your place.\\nSo make this the goal to which you aspire. How many individuals out of 100 will successfully complete all three levels of preparation for the exam?', 'Assistant Response': 'Based on the given information, out of 100 individuals who start, only 20 will make it to the end. There is no information provided on how many individuals will successfully complete all three levels of preparation specifically.'}] \n",
      "\n",
      "\n",
      "********************\n",
      "example 2 : {'messages': [{'content': \"Can you translate the text material into Spanish or any other language?: He really is, you know.\\nThings a hero should show.\\nHe loves me more than a zillion things.\\nHe loves me when I sing my jolly folktale rhymes.\\nHe's good, not just good, in fact he's great!\\nBut because he's my best mate!\\nWOW !!! I love it!!!!\", 'role': 'user'}, {'content': '¿Puede traducir el texto a español o a cualquier otro idioma?: \\nRealmente lo es, ya sabes.\\nCosas que un héroe debería demostrar.\\nMe quiere más que un millón de cosas.\\nMe quiere cuando canto mis alegres rimas de cuentos populares.\\nEs bueno, no solo bueno, ¡de hecho es genial!\\n¡Pero porque es mi mejor amigo!\\n¡WOW! ¡Me encanta!', 'role': 'assistant'}]}\n",
      "\n",
      "user_msg: Can you translate the text material into Spanish or any other language?: He really is, you know.\n",
      "Things a hero should show.\n",
      "He loves me more than a zillion things.\n",
      "He loves me when I sing my jolly folktale rhymes.\n",
      "He's good, not just good, in fact he's great!\n",
      "But because he's my best mate!\n",
      "WOW !!! I love it!!!! \n",
      "\n",
      "assistant_msg: ¿Puede traducir el texto a español o a cualquier otro idioma?: \n",
      "Realmente lo es, ya sabes.\n",
      "Cosas que un héroe debería demostrar.\n",
      "Me quiere más que un millón de cosas.\n",
      "Me quiere cuando canto mis alegres rimas de cuentos populares.\n",
      "Es bueno, no solo bueno, ¡de hecho es genial!\n",
      "¡Pero porque es mi mejor amigo!\n",
      "¡WOW! ¡Me encanta! \n",
      "\n",
      "rows: [{'User Prompt': \"- The left child should have a value less than the parent node's value, and the right child should have a value greater than the parent node's value.\", 'Assistant Response': \"This statement is correct. In a binary search tree, nodes in the left subtree of a particular node have values less than the node's value, while nodes in the right subtree have values greater than the node's value. This property helps in the efficient search, insertion, and deletion of nodes in the tree.\"}, {'User Prompt': 'To pass three levels must be the plan.\\nThen tackle Two, when that is done.\\nOf 100 that start, at the end will be 20.\\nFinQuiz is a website that helps you prepare.\\nUse it to be stress-free, and not lose your hair.\\nThen, take the exam with a smile on your face.\\nBe confident that you will gain your place.\\nSo make this the goal to which you aspire. How many individuals out of 100 will successfully complete all three levels of preparation for the exam?', 'Assistant Response': 'Based on the given information, out of 100 individuals who start, only 20 will make it to the end. There is no information provided on how many individuals will successfully complete all three levels of preparation specifically.'}, {'User Prompt': \"Can you translate the text material into Spanish or any other language?: He really is, you know.\\nThings a hero should show.\\nHe loves me more than a zillion things.\\nHe loves me when I sing my jolly folktale rhymes.\\nHe's good, not just good, in fact he's great!\\nBut because he's my best mate!\\nWOW !!! I love it!!!!\", 'Assistant Response': '¿Puede traducir el texto a español o a cualquier otro idioma?: \\nRealmente lo es, ya sabes.\\nCosas que un héroe debería demostrar.\\nMe quiere más que un millón de cosas.\\nMe quiere cuando canto mis alegres rimas de cuentos populares.\\nEs bueno, no solo bueno, ¡de hecho es genial!\\n¡Pero porque es mi mejor amigo!\\n¡WOW! ¡Me encanta!'}] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User Prompt</th>\n",
       "      <th>Assistant Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- The left child should have a value less than the parent node's value, and the right child should have a value greater than the parent node's value.</td>\n",
       "      <td>This statement is correct. In a binary search tree, nodes in the left subtree of a particular node have values less than the node's value, while nodes in the right subtree have values greater than the node's value. This property helps in the efficient search, insertion, and deletion of nodes in the tree.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To pass three levels must be the plan.\\nThen tackle Two, when that is done.\\nOf 100 that start, at the end will be 20.\\nFinQuiz is a website that helps you prepare.\\nUse it to be stress-free, and not lose your hair.\\nThen, take the exam with a smile on your face.\\nBe confident that you will gain your place.\\nSo make this the goal to which you aspire. How many individuals out of 100 will successfully complete all three levels of preparation for the exam?</td>\n",
       "      <td>Based on the given information, out of 100 individuals who start, only 20 will make it to the end. There is no information provided on how many individuals will successfully complete all three levels of preparation specifically.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can you translate the text material into Spanish or any other language?: He really is, you know.\\nThings a hero should show.\\nHe loves me more than a zillion things.\\nHe loves me when I sing my jolly folktale rhymes.\\nHe's good, not just good, in fact he's great!\\nBut because he's my best mate!\\nWOW !!! I love it!!!!</td>\n",
       "      <td>¿Puede traducir el texto a español o a cualquier otro idioma?: \\nRealmente lo es, ya sabes.\\nCosas que un héroe debería demostrar.\\nMe quiere más que un millón de cosas.\\nMe quiere cuando canto mis alegres rimas de cuentos populares.\\nEs bueno, no solo bueno, ¡de hecho es genial!\\n¡Pero porque es mi mejor amigo!\\n¡WOW! ¡Me encanta!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                 User Prompt  \\\n",
       "0                                                                                                                                                                                                                                                                                                                      - The left child should have a value less than the parent node's value, and the right child should have a value greater than the parent node's value.   \n",
       "1  To pass three levels must be the plan.\\nThen tackle Two, when that is done.\\nOf 100 that start, at the end will be 20.\\nFinQuiz is a website that helps you prepare.\\nUse it to be stress-free, and not lose your hair.\\nThen, take the exam with a smile on your face.\\nBe confident that you will gain your place.\\nSo make this the goal to which you aspire. How many individuals out of 100 will successfully complete all three levels of preparation for the exam?   \n",
       "2                                                                                                                                             Can you translate the text material into Spanish or any other language?: He really is, you know.\\nThings a hero should show.\\nHe loves me more than a zillion things.\\nHe loves me when I sing my jolly folktale rhymes.\\nHe's good, not just good, in fact he's great!\\nBut because he's my best mate!\\nWOW !!! I love it!!!!   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                              Assistant Response  \n",
       "0                              This statement is correct. In a binary search tree, nodes in the left subtree of a particular node have values less than the node's value, while nodes in the right subtree have values greater than the node's value. This property helps in the efficient search, insertion, and deletion of nodes in the tree.  \n",
       "1                                                                                                           Based on the given information, out of 100 individuals who start, only 20 will make it to the end. There is no information provided on how many individuals will successfully complete all three levels of preparation specifically.  \n",
       "2  ¿Puede traducir el texto a español o a cualquier otro idioma?: \\nRealmente lo es, ya sabes.\\nCosas que un héroe debería demostrar.\\nMe quiere más que un millón de cosas.\\nMe quiere cuando canto mis alegres rimas de cuentos populares.\\nEs bueno, no solo bueno, ¡de hecho es genial!\\n¡Pero porque es mi mejor amigo!\\n¡WOW! ¡Me encanta!  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load the SFT training dataset from Hugging Face hub:\n",
    "train_dataset = load_dataset(\"banghua/DL-SFT-Dataset\")[\"train\"]\n",
    "\n",
    "if not USE_GPU:\n",
    "    train_dataset=train_dataset.select(range(100))\n",
    "\n",
    "display_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01c515a8-3728-45fa-88cc-6eb4de839839",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "# SFTTrainer config\n",
    "#Defines how training should be run, including learning rate, epochs, batch size, etc:\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    learning_rate=8e-5, # Learning rate for training. \n",
    "    num_train_epochs=1, #  Set the number of epochs to train the model.\n",
    "    per_device_train_batch_size=1, # Batch size for each device (e.g., GPU) during training. \n",
    "    gradient_accumulation_steps=8, # Number of steps before performing a backward/update pass to accumulate gradients.\n",
    "    gradient_checkpointing=False, # Enable gradient checkpointing to reduce memory usage during training at the cost of slower training speed.\n",
    "    logging_steps=2,  # Frequency of logging training progress (log every 2 steps).\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cd7d132-4c69-4b12-a8ec-e6b4795faad9",
   "metadata": {
    "height": 132
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b44586ba9984488f83ad52e411f5340d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:45, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.608600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.423400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.360500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.126100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.191000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.075100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12, training_loss=2.297439376513163, metrics={'train_runtime': 49.3189, 'train_samples_per_second': 2.028, 'train_steps_per_second': 0.243, 'total_flos': 10120976949888.0, 'train_loss': 2.297439376513163, 'epoch': 0.96})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_trainer = SFTTrainer(\n",
    "    model=model, #The base model to be fine-tuned.\n",
    "    args=sft_config, #Training arguments defined above.\n",
    "    train_dataset=train_dataset, #Dataset to fine-tune the model on.\n",
    "    processing_class=tokenizer, #Tokenizer used for processing input text.\n",
    ")\n",
    "sft_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26906aad",
   "metadata": {},
   "source": [
    "- global_step: Total number of optimizer steps (i.e., weight updates) performed during training.\n",
    "- training_loss: Final averaged training loss over all global steps.\n",
    "- train_runtime: Total time taken to complete training, in seconds.\n",
    "- train_samples_per_second: Number of training examples processed per second.\n",
    "- train_steps_per_second: Number of optimizer steps (i.e., gradient updates) performed per second.\n",
    "- total_flos: Estimated number of floating point operations used during training.\n",
    "- epoch: Portion of the training epoch completed (1.0 = full dataset seen once)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b4bac1-7262-4c55-b411-6a59188157b0",
   "metadata": {},
   "source": [
    "## Testing training results on small model and small dataset\n",
    "\n",
    "**Note:** The following results are for the small model and dataset we used for SFT training, due to limited computational resources. To view the results of full-scale training on a larger model, see the **\"SFT Results on Qwen3-0.6B Model\"** section above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d678274-5768-4cea-ae20-051488e5d0f3",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Base Model (After SFT) Output ===\n",
      "\n",
      "Model Input 1:\n",
      "Give me an 1-sentence introduction of LLM.\n",
      "Model Output 1:\n",
      "The course is designed to provide students with a solid foundation in the theory and practice of law. The course is designed to provide students with a solid foundation in the theory and practice of law. The course is designed to provide students with a solid foundation in the theory and practice of law. The course is designed to provide students with a solid foundation in the theory and practice of law. The course is designed to provide students with a solid foundation in the theory and practice of law. The course is designed\n",
      "\n",
      "\n",
      "Model Input 2:\n",
      "Calculate 1+1-1\n",
      "Model Output 2:\n",
      "1+1-1 = 1+1-1 = 1+1-1 = 1+1-1 = 1+1-1 = 1+1-1 = 1+1-1 = 1+1-1 = 1+1-1 = 1+1-1 = 1+1-1 = 1+1-1 = 1+1-1 = 1+1-1 = 1+\n",
      "\n",
      "\n",
      "Model Input 3:\n",
      "What's the difference between thread and process?\n",
      "Model Output 3:\n",
      "Thread is a single process that is running in a single process space. A thread is a single process that is running in a single process space. A thread is a single process that is running in a single process space. A thread is a single process that is running in a single process space. A thread is a single process that is running in a single process space. A thread is a single process that is running in a single process space. A thread is a single process that is running in\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not USE_GPU: #moving model to CPU when GPU isn’t requested:\n",
    "    sft_trainer.model.to(\"cpu\")\n",
    "\n",
    "#Evaluating the fine-tuned model’s performance on the same test questions:\n",
    "test_model_with_questions(sft_trainer.model, tokenizer, questions, \n",
    "                          title=\"Base Model (After SFT) Output\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
